{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOznXsyA5EQ90ZwGOvNHr5K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yenchunR/NLP_HW4/blob/master/HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9IOSIgvNeBU"
      },
      "source": [
        "# Install Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ_DqNfo0Wfd",
        "outputId": "06c0c97b-70ae-4638-9042-149d92eddc71"
      },
      "source": [
        "!pip3 install hanziconv"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hanziconv\n",
            "  Downloading hanziconv-0.3.2.tar.gz (276 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |██████                          | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 92 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276 kB 5.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: hanziconv\n",
            "  Building wheel for hanziconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hanziconv: filename=hanziconv-0.3.2-py2.py3-none-any.whl size=23225 sha256=51fb4f72de77b86b5812f5b76df9edda70e2e32c2f2d7d83c557d9b6e1a88ba5\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e3/22/7bf50146a3ee95d1fdcbfabc44a1fe15b6e2ab7348ab7337bf\n",
            "Successfully built hanziconv\n",
            "Installing collected packages: hanziconv\n",
            "Successfully installed hanziconv-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlGwD3cYYAkF",
        "outputId": "682b9419-e37a-481d-9d68-056d52de5d10"
      },
      "source": [
        "!pip3 install gensim"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF7ZwfOKpdEw"
      },
      "source": [
        "# 將Colab連結Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D3f_ouRpj2B",
        "outputId": "99a26a1d-cc6f-4410-ace1-41422f0a7e0e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kia8HbibsPxt"
      },
      "source": [
        "# 讀取所有檔案路徑"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyrJilquqGK0",
        "outputId": "a5dcd525-9618-4391-bddd-2fbf6a7c3260"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 把列出資料夾的程式碼寫成一個函式\n",
        "filePath = []\n",
        "def show_folder_content(folder_path):\n",
        "\n",
        "    folder_content = os.listdir(folder_path)\n",
        "    for item in folder_content:\n",
        "        if os.path.isdir(folder_path + '/' + item):\n",
        "            # 呼叫自己處理這個子資料夾\n",
        "            show_folder_content(folder_path + '/' + item)\n",
        "        elif os.path.isfile(folder_path + '/' + item):\n",
        "            filePath.append(folder_path + '/' + item)\n",
        "        else:\n",
        "            print('無法辨識：' + item)\n",
        "\n",
        "\n",
        "# 顯示指定資料夾的內容\n",
        "target_folder = '/content/drive/My Drive/wiki_zh'\n",
        "show_folder_content(target_folder)\n",
        "print(\"Finish\", len(filePath))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finish 1280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_br9dhQMzVdj"
      },
      "source": [
        "# jieba 斷詞"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3Jcdxzzwger"
      },
      "source": [
        "# coding=utf-8\n",
        "import jieba \n",
        "import json\n",
        "import re\n",
        "\n",
        "# 讀取停用詞\n",
        "def stopwordslist(filepath):  \n",
        "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]  \n",
        "    return stopwords \n",
        "\n",
        "fileTrainSeg = [] #斷好詞的結果\n",
        "\n",
        "def LoadToSeg(filepath, fW):\n",
        "    with open(filePath[0], 'r') as obj:\n",
        "        for line in obj.readlines():\n",
        "            dic = json.loads(line)\n",
        "            temp_text = re.findall('[\\u4e00-\\u9fa5]', dic['text'])\n",
        "            text = ''.join(temp_text)\n",
        "            tags = jieba.lcut(text, cut_all=False)\n",
        "            ThisLine = ''\n",
        "            for tag in tags:\n",
        "                if tag not in stopwords:  \n",
        "                    if tag != '\\t' and tag != '\\n' and tag != '' and len(tag) > 1:  \n",
        "                        ThisLine += tag  \n",
        "                        ThisLine += \" \"   #再次組合成【帶空格】的串\n",
        "            fW.write((ThisLine+\"\\n\").encode('utf-8'))\n",
        "            \n",
        "stopwords = stopwordslist('/content/drive/My Drive/wiki_zh/DriveUploader/cn_stopwords.txt')  # 這裏加載停用詞的路徑 \n",
        "fileSegWordDonePath ='/content/drive/My Drive/wiki_zh/DriveUploader/corpusSegDone.txt'\n",
        "with open(fileSegWordDonePath,'wb') as fW:\n",
        "    fW.truncate(0)\n",
        "    index = 0\n",
        "    for path in filePath:\n",
        "        LoadToSeg(path, fW)\n",
        "        index += 1\n",
        "\n",
        "print(\"Finish\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSDjMZmDNvbQ"
      },
      "source": [
        "# word2vec 轉成高維空間向量"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmFlgeRENyJw",
        "outputId": "6496db6d-b8a9-4210-f0f5-611a671412a2"
      },
      "source": [
        "from gensim.models import word2vec\n",
        "# jieba分詞轉word2vec向量\n",
        "fileSegWordDonePath ='/content/drive/My Drive/wiki_zh/DriveUploader/corpusSegDone.txt'\n",
        "fileSegWordDonePath2 ='/content/drive/My Drive/wiki_zh/DriveUploader/corpusSegDone.model'\n",
        "\n",
        "sentences = word2vec.LineSentence(fileSegWordDonePath)\n",
        "model = word2vec.Word2Vec(sentences, size=250, min_count=5, negative=5, workers=3, sg=1)\n",
        "model.save(fileSegWordDonePath2)\n",
        "print(\"Finish\")"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "cG3wID4jRxH_",
        "outputId": "fa44858d-86da-4c9c-ef0d-fd841ba57755"
      },
      "source": [
        "# -*- coding: utf-8 -*-# -*- coding: utf-8 -*-\n",
        "import sys\n",
        "from gensim.models import word2vec \n",
        "from gensim import models\n",
        "from hanziconv import HanziConv\n",
        "\n",
        "def main():   \n",
        "    a = input(\"input:\") \n",
        "    a = HanziConv.toSimplified(a)\n",
        "    model = models.Word2Vec.load(fileSegWordDonePath2)\n",
        "    res = model.most_similar(a)\n",
        "    for item in res: \n",
        "        print(HanziConv.toTraditional(item[0]) + ':' + str(item[1]))\n",
        "\n",
        "if __name__=='__main__':\n",
        "    main()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:李知恩\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-efab6d277b76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-117-efab6d277b76>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHanziConv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSimplified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileSegWordDonePath2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHanziConv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoTraditional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m':'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1420\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m                 )\n\u001b[0;32m-> 1422\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         \"\"\"\n\u001b[0;32m-> 1397\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word '李知恩' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8uhRbtlORPM"
      },
      "source": [
        "# Result"
      ]
    }
  ]
}
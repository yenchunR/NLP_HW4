{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWW4YoakTKsTzUk3NPINqN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yenchunR/NLP_HW4/blob/master/HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9IOSIgvNeBU"
      },
      "source": [
        "# Install Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ_DqNfo0Wfd",
        "outputId": "06c0c97b-70ae-4638-9042-149d92eddc71"
      },
      "source": [
        "!pip3 install hanziconv"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hanziconv\n",
            "  Downloading hanziconv-0.3.2.tar.gz (276 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 19.7 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |██████                          | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 92 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276 kB 5.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: hanziconv\n",
            "  Building wheel for hanziconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hanziconv: filename=hanziconv-0.3.2-py2.py3-none-any.whl size=23225 sha256=51fb4f72de77b86b5812f5b76df9edda70e2e32c2f2d7d83c557d9b6e1a88ba5\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e3/22/7bf50146a3ee95d1fdcbfabc44a1fe15b6e2ab7348ab7337bf\n",
            "Successfully built hanziconv\n",
            "Installing collected packages: hanziconv\n",
            "Successfully installed hanziconv-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlGwD3cYYAkF",
        "outputId": "682b9419-e37a-481d-9d68-056d52de5d10"
      },
      "source": [
        "!pip3 install gensim"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF7ZwfOKpdEw"
      },
      "source": [
        "# 將Colab連結Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D3f_ouRpj2B",
        "outputId": "99a26a1d-cc6f-4410-ace1-41422f0a7e0e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kia8HbibsPxt"
      },
      "source": [
        "# 讀取所有檔案路徑"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyrJilquqGK0",
        "outputId": "a5dcd525-9618-4391-bddd-2fbf6a7c3260"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 把列出資料夾的程式碼寫成一個函式\n",
        "filePath = []\n",
        "def show_folder_content(folder_path):\n",
        "\n",
        "    folder_content = os.listdir(folder_path)\n",
        "    for item in folder_content:\n",
        "        if os.path.isdir(folder_path + '/' + item):\n",
        "            # 呼叫自己處理這個子資料夾\n",
        "            show_folder_content(folder_path + '/' + item)\n",
        "        elif os.path.isfile(folder_path + '/' + item):\n",
        "            filePath.append(folder_path + '/' + item)\n",
        "        else:\n",
        "            print('無法辨識：' + item)\n",
        "\n",
        "\n",
        "# 顯示指定資料夾的內容\n",
        "target_folder = '/content/drive/My Drive/wiki_zh'\n",
        "show_folder_content(target_folder)\n",
        "print(\"Finish\", len(filePath))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finish 1280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_br9dhQMzVdj"
      },
      "source": [
        "# jieba 斷詞"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "id": "w3Jcdxzzwger",
        "outputId": "7ac3d7e6-4e65-4c5d-f130-aa9f04f1420b"
      },
      "source": [
        "# coding=utf-8\n",
        "import jieba \n",
        "from hanziconv import HanziConv\n",
        "import json\n",
        "import re\n",
        "\n",
        "# 讀取停用詞\n",
        "def stopwordslist(filepath):  \n",
        "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]  \n",
        "    return stopwords \n",
        "\n",
        "fileTrainSeg = [] #斷好詞的結果\n",
        "\n",
        "def LoadToSeg(filepath, fW):\n",
        "    with open(filePath[0], 'r') as obj:\n",
        "        for line in obj.readlines():\n",
        "            dic = json.loads(line)\n",
        "            temp_text = re.findall('[\\u4e00-\\u9fa5]', dic['text'])\n",
        "            text = ''.join(temp_text)\n",
        "            tags = jieba.lcut(text, cut_all=False)\n",
        "            ThisLine = ''\n",
        "            for tag in tags:\n",
        "                if tag not in stopwords:  \n",
        "                    if tag != '\\t' and tag != '\\n' and tag != '' and len(tag) > 1:  \n",
        "                        ThisLine += tag  \n",
        "                        ThisLine += \" \"   #再次組合成【帶空格】的串\n",
        "            fW.write((ThisLine+\"\\n\").encode('utf-8'))\n",
        "            \n",
        "stopwords = stopwordslist('/content/drive/My Drive/wiki_zh/DriveUploader/cn_stopwords.txt')  # 這裏加載停用詞的路徑 \n",
        "\n",
        "with open(fileSegWordDonePath,'wb') as fW:\n",
        "    \n",
        "    index = 0\n",
        "    for path in filePath:\n",
        "        LoadToSeg(path, fW)\n",
        "        index += 1\n",
        "        print(\"Start:\", index)\n",
        "\n",
        "fileSegWordDonePath ='/content/drive/My Drive/wiki_zh/DriveUploader/corpusSegDone.txt'\n",
        "        \n",
        "#print(fileTrainSeg[0])\n",
        "print(\"Finish\")"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start: 1\n",
            "Start: 2\n",
            "Start: 3\n",
            "Start: 4\n",
            "Start: 5\n",
            "Start: 6\n",
            "Start: 7\n",
            "Start: 8\n",
            "Start: 9\n",
            "Start: 10\n",
            "Start: 11\n",
            "Start: 12\n",
            "Start: 13\n",
            "Start: 14\n",
            "Start: 15\n",
            "Start: 16\n",
            "Start: 17\n",
            "Start: 18\n",
            "Start: 19\n",
            "Start: 20\n",
            "Start: 21\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-231be10561e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilePath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mLoadToSeg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-110-231be10561e6>\u001b[0m in \u001b[0;36mLoadToSeg\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mtemp_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[\\u4e00-\\u9fa5]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcut_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mThisLine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jieba/__init__.py\u001b[0m in \u001b[0;36mlcut\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlcut_for_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jieba/__init__.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(self, sentence, cut_all, HMM, use_paddle)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mre_han\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcut_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jieba/__init__.py\u001b[0m in \u001b[0;36m__cut_DAG\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    267\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFREQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                             \u001b[0mrecognized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinalseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecognized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                                 \u001b[0;32myield\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jieba/finalseg/__init__.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mre_han\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__cut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mForce_Split_Words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSDjMZmDNvbQ"
      },
      "source": [
        "# word2vec 轉成高維空間向量"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmFlgeRENyJw",
        "outputId": "21c8e02f-10f6-4fa0-86f3-5dd1350b4cc5"
      },
      "source": [
        "from gensim.models import word2vec\n",
        "# jieba分詞轉word2vec向量\n",
        "fileSegWordDonePath ='/content/drive/My Drive/wiki_zh/DriveUploader/corpusSegDone.txt'\n",
        "fileSegWordDonePath2 ='/content/drive/My Drive/wiki_zh/DriveUploader/corpusSegDone.model'\n",
        "\n",
        "sentences = word2vec.LineSentence(fileSegWordDonePath)\n",
        "model = word2vec.Word2Vec(sentences, size=5, min_count=1, negative=10)\n",
        "model.save(fileSegWordDonePath2)\n",
        "print(\"Finish\")"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG3wID4jRxH_",
        "outputId": "ccfd208b-f173-4c0e-a100-b066344fa5f2"
      },
      "source": [
        "# -*- coding: utf-8 -*-# -*- coding: utf-8 -*-\n",
        "import sys\n",
        "from gensim.models import word2vec \n",
        "from gensim import models\n",
        "def main():   \n",
        "    a = input(\"input:\") \n",
        "    model = models.Word2Vec.load(fileSegWordDonePath2)\n",
        "    res = model.most_similar(a)\n",
        "    for item in res: \n",
        "        print(item[0] + ':' + str(item[1]))\n",
        "\n",
        "if __name__=='__main__':\n",
        "    main()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input:苏拉\n",
            "长米:0.9986809492111206\n",
            "亚平宁:0.9979071021080017\n",
            "右侧:0.997556746006012\n",
            "圣佩德罗:0.9955615401268005\n",
            "享有:0.9943423271179199\n",
            "台阶:0.99310302734375\n",
            "勒姆:0.992180347442627\n",
            "彭尼诺山:0.9917698502540588\n",
            "加尔默罗:0.9905072450637817\n",
            "山峰:0.9903616905212402\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8uhRbtlORPM"
      },
      "source": [
        "# Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "X-ZiA4JoOTcn",
        "outputId": "4d8f473c-e3ea-4863-ac38-c15adc8ae6d6"
      },
      "source": [
        "\n",
        "model = word2vec.load(fileSegWordDonePath2)\n",
        "indexes = model.cosine(u'苏拉') # 此字詞有出現在corpusWord2Vec.bin當中\n",
        "for index in indexes[0]:\n",
        "    print(model.vocab[index])"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-e755e371fb05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileSegWordDonePath2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'苏拉'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 此字詞有出現在corpusWord2Vec.bin當中\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/word2vec/io.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fname, kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not identify kind\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"bin\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"txt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/word2vec/wordvectors.py\u001b[0m in \u001b[0;36mfrom_binary\u001b[0;34m(cls, fname, vocab_unicode_size, desired_vocab, encoding, new_lines)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0minclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesired_vocab\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdesired_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                     \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m# read vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x90 in position 0: invalid start byte"
          ]
        }
      ]
    }
  ]
}